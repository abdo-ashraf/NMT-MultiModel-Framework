# NMT-MultiModel-Training-Framework
**NMT-MultiModel-Training-Framework** is a versatile and scalable framework designed for training and evaluating Neural Machine Translation (NMT) models using multiple architectures. This framework supports various NMT models, including but not limited to Seq2Seq, Transformer, and Attention-based models. It is built to facilitate easy experimentation, customization, and deployment of NMT systems.

Whether you're a researcher exploring new NMT architectures or a developer building production-ready translation systems, this framework provides the tools and flexibility to meet your needs.

---

## Features

- **Multi-Model Support**: Train and evaluate multiple NMT architectures (e.g., Seq2Seq, Transformer) within a single framework.
- **Customizable Configurations**: Easily configure model hyperparameters, data preprocessing, and training pipelines.
- **Data Preprocessing Tools**: Built-in tools for tokenization, batching, and dataset preparation.
- **Evaluation Metrics**: Compute standard NMT evaluation metrics such as BLEU.
- **Scalable Training**: Supports training on both CPU and GPU, with options for distributed training.
- **Extensible Design**: Modular codebase for adding new models, datasets, or evaluation metrics.

---

## Installation

### Prerequisites

- Python 3.11 or higher
- PyTorch (>=  2.4.1)
- Other dependencies listed in `requirements.txt`

### Steps
1. Clone the repository:
   ```bash
   git clone https://github.com/abdo-ashraf/NMT-MultiModel-Training-Framework.git
   cd NMT-MultiModel-Training-Framework
   ```
2. Install the required dependencies:
   ```bash
   make setup
   ```
## Deployment Link for: https://huggingface.co/spaces/TheDemond/Neural-machine-translation
